{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Data Preprocessing Skills\n",
    "\n",
    "Before we starting data preprocessing, please make sure you have installed following dependencies:\n",
    "- json\n",
    "- numpy\n",
    "- pandas\n",
    "- matplotlib\n",
    "- h5py\n",
    "- gzip\n",
    "- tqdm  (optional)\n",
    "\n",
    "You can install them by typing `pip install xxx` in terminal.\n",
    "\n",
    "*If you are using conda as your virtual environment platform, `conda install xxx` would be a better choice for installing dependencies. Use `pip install` if **conda** cannot find such packages*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load & Save various data types\n",
    "\n",
    "During our study on Particle Physics, data may come with different types, such as .h5 , .pkl , .json , .txt , .cvs. And it might be even compacted as .g or .gz. So our first step is to read them. <br>\n",
    "When we read data, pandas.DataFrame is always our best friend. We also sometimes use numpy.ndarray. <br>\n",
    "\n",
    "The read_file functions have plenty of parameters, you can check it yourself. Here I provide the <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_hdf.html\">link</a> to pandas.DataFrame.read_hdf()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Load Files\n",
    "\n",
    "#### Uncompacted Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import gzip\n",
    "\n",
    "load_path = \"data/samples\"\n",
    "df1 = pd.read_hdf(load_path+\".h5\", key=\"data\")          \n",
    "# I know the key is \"data\", because I created it. \n",
    "# For unfamiliar data, you need to figure the key out first.\n",
    "df2 = pd.read_pickle(load_path+'.pkl')\n",
    "df3 = pd.read_json(load_path+'.json')\n",
    "df4 = pd.read_csv(load_path+'.csv', sep=',')\n",
    "df5 = pd.read_csv(load_path+'.txt', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find out the key of a .h5 file, you need to read it by h5py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['data']>\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(load_path+'.h5','r') as f:\n",
    "    print(f.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 79)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check your data shape, it should be (20k,79)\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compacted Files"
   ]
  },
  {
   "source": [
    "### 1) Load Files\n",
    "#### Uncompacted Files"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(load_path+'.txt.gz', 'r') as f:\n",
    "    df6 = pd.read_csv(f,sep=',', )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For .z files, you can directly load it by h5py.File()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Save Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = 'data/saved/samples'\n",
    "# Uncompacted\n",
    "df1.to_hdf(out_dir+'.h5', key=\"data\")\n",
    "df1.to_pickle(out_dir+'.pkl')\n",
    "df1.to_json(out_dir+'.json')\n",
    "df1.to_csv(out_dir+\".csv\", sep=',', header=True,index=False)\n",
    "df1.to_csv(out_dir+\".txt\", sep=',', header=True,index=False)\n",
    "# Compacted\n",
    "df1.to_csv(out_dir+\".txt.gz\", sep=',', header=True, compression='gzip',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, we tend to use .h5, because it takes up relatively small space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Excercise\n",
    "Try them yourself and check whether the 6 DataFrames are the same. You can check their shapes, column names, etc.\n",
    "If you don't know how to get those information, ask Google!\n",
    "\n",
    "Then you should try save your data into different types, you can change the key for .h5, or other paramters."
   ]
  },
  {
   "source": [
    "## 2. Learning Features \n",
    "### 1) Low-level Features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "|Features|Description|\n",
    "|:--|:--|\n",
    "j1_pt| transversal momentum\n",
    "j1_ptrel| ratio of the pT of each consistent to the pT of the jet \n",
    "j1_e| energy\n",
    "j1_erel| ratio of the energy of each consistent to the pT of the jet\n",
    "j1_eta| pseudorapidity\n",
    "j1_phi| azimuthal angle\n",
    "j1_etarot| rotated eta of each constituent  \n",
    "j1_phirot| rotated phi of each constituent \n",
    "j1_deltaR| sqrt((Δeta)2 + (Δ phi)2 ) \n",
    "j1_costhetarel| cos (  angle (constituent, jet) )   \n",
    "j1_pdgid| PDF ID number of the constituent\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 2) High-level Features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "|Features|Description|\n",
    "|:--|:--|\n",
    "'j_ptfrac'| Ratio of jet pT to the event\n",
    "'j_pt'| jet pT (transverse momentum)\n",
    "'j_eta'| jet pseudo rapidity\n",
    "'j_mass'| jet mass \n",
    "‘'j_tau1_b1', 'j_tau2_b1', 'j_tau3_b1', 'j_tau1_b2', 'j_tau2_b2', 'j_tau3_b2', 'j_tau32_b1', 'j_tau32_b2'| N-subjetiness which can measure N-prone sub-structure\n",
    "'j_zlogz'| Jet splitting fraction\n",
    "'j_c1_b0', 'j_c1_b1', 'j_c1_b2', 'j_c2_b1', 'j_c2_b2', 'j_d2_b1', 'j_d2_b2', 'j_d2_a1_b1', 'j_d2_a1_b2', 'j_m2_b1', 'j_m2_b2', 'j_n2_b1', 'j_n2_b2'| Energy Correlation Function\n",
    "'j_tau1_b1_mmdt', 'j_tau2_b1_mmdt', 'j_tau3_b1_mmdt', 'j_tau1_b2_mmdt', 'j_tau2_b2_mmdt', 'j_tau3_b2_mmdt', 'j_tau32_b1_mmdt', 'j_tau32_b2_mmdt'| N-subjetiness computed after modified mass drop tagger\n",
    "'j_c1_b0_mmdt', 'j_c1_b1_mmdt', 'j_c1_b2_mmdt', 'j_c2_b1_mmdt', 'j_c2_b2_mmdt', 'j_d2_b1_mmdt', 'j_d2_b2_mmdt', 'j_d2_a1_b1_mmdt', 'j_d2_a1_b2_mmdt', 'j_m2_b1_mmdt', 'j_m2_b2_mmdt', 'j_n2_b1_mmdt', 'j_n2_b2_mmdt'| energy correlation function computed based on modified mass drop tagger\n",
    "'j_mass_trim'| trimmed jet mass\n",
    "'j_mass_mmdt'| jet mass computed based on modified mass drop tagger\n",
    "'j_mass_prun'| pruned jet mass\n",
    "'j_mass_sdb2'| soft-drop b2 jet mass\n",
    "'j_mass_sdm1'| soft-drop m1 jet mass\n",
    "'j_multiplicity'| number of constituents\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 3) Excercise\n",
    "Try to plot 1D features and compare the distribution between labels. Hint: numpy.historgram()"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3. Jet Image Plotting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image-based neural networks are common in machine learning. After we processed the jets into images, we can implement different image-based neural networks such as Convolutional Neural Networks 2D(CNN2D), ResNet-50."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `samples.csv` as our example data for practicing jet image processing. Take what we need from the data:\n",
    "\n",
    "Inputs features:\n",
    "- j1_ptrel\n",
    "- j1_etarot\n",
    "- j1_phirot\n",
    "- j_index\n",
    "    \n",
    "Labels:\n",
    "- j_g\n",
    "- j_q\n",
    "- j_w\n",
    "- j_z\n",
    "- j_t\n",
    "- j_index\n",
    "\n",
    "We put `j_index` in both categories since we need it to identify where the constituent is in which jet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path = \"data/samples\"\n",
    "df = pd.read_csv(load_path+'.csv', sep=',')\n",
    "features = ['j1_ptrel', 'j1_etarot', 'j1_phirot', 'j_index']\n",
    "labels = ['j_g', 'j_q', 'j_w', 'j_z', 'j_t', 'j_index']\n",
    "features_labels_df = df[list(set(features+labels))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we separate the data into two categories, input features and jet labels, which correspond to X and y in later traning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = features_labels_df[features]\n",
    "labels_df = features_labels_df[labels]\n",
    "# Convert to numpy array \n",
    "features_val = features_df.values\n",
    "labels_val = labels_df.values\n",
    "if 'j_index' in features:\n",
    "    features_val = features_val[:,:-1] # drop the j_index feature\n",
    "if 'j_index' in labels:\n",
    "    labels_val = labels_val[:,:-1] # drop the j_index label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert jet into a 2D image, which in this case, will be a 40x40 image. The range in horizontal axis is from -0.8 to 0.8 and that in vertical axis is from -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BinsX = 40\n",
    "BinsY = 40\n",
    "MinX = -0.8\n",
    "MaxX = 0.8\n",
    "MinY = -1.0\n",
    "MaxY = 1.0\n",
    "features_2dval = np.zeros((len(labels_df), BinsX, BinsY, 1))\n",
    "for i in range(0, len(labels_df)):\n",
    "    features_df_i = features_df[features_df['j_index']==labels_df['j_index'].iloc[i]]\n",
    "    index_values = features_df_i.index.values\n",
    "\n",
    "    xbins = np.linspace(MinX, MaxX, BinsX+1)\n",
    "    ybins = np.linspace(MinY, MaxY, BinsY+1)\n",
    "\n",
    "    x = features_df_i[features[0]] # horizontal location\n",
    "    y = features_df_i[features[1]] # vertical location\n",
    "    w = features_df_i[features[2]] # weight at that location\n",
    "\n",
    "    hist, xedges, yedges = np.histogram2d(x, y, weights=w, bins=(xbins,ybins)) # Use np.histogram2d to plot the image\n",
    "\n",
    "    for ix in range(0, BinsX):\n",
    "        for iy in range(0, BinsY):\n",
    "            features_2dval[i,ix,iy,0] = hist[ix,iy] # put the image into the array\n",
    "features_val = features_2dval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check `features_val` shape, which will be `(20000, 40, 40, 1)`, corresponds to `(total_jets, horizontal_bins, vertical_bins, one_image_per_jet)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 40, 40, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train_test_split` is a method in scikit-learn package to split the data into two parts, one for training and another for testing the training result. There are 4 output parameters for this method. X_train, X_test, y_train, y_test. For detailed explanation, click this [link](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html?highlight=train_test_split#sklearn.model_selection.train_test_split) to check documentation.\n",
    "\n",
    "In linear regressing, we use X, the independent variables or the predictors, to predict y, the dependent variable or the response. In our case, X is the input features and y is the labels. We strongly recommand that 4 output parameters should be named in such scheme.\n",
    "\n",
    "`test_size` is to determine the portion that split into X_test and y_test. We choose `random_state` to be 42, so that the data will always split in the same way as the last time we use this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_val, labels_val, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check shape for all 4 output parameters to see how changing test_size would affect the X_test and y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000, 40, 40, 1)\n",
      "(4000, 40, 40, 1)\n",
      "(16000, 5)\n",
      "(4000, 5)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have all your data prepared for training in neural networks, try to plot the heatmap for all 5 labels before we head to the training process. Remember, every entry in X has a corresponding entry in y.\n",
    "\n",
    "You can find multiple ways to plot the heatmap, `matplotlib` would be a good package to use as start point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}